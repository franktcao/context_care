# context_care


# Answers to Questions
1. 3 takeaways for this research are:
    1. Multiple knowledge bases and eletronic health record (EHR) systems are
    brought together to construct a knowledge graph (KG) of clinical data and
    embeddings of clinical vocabulary in context.

    1. The KG, which defines the graph neural network architecture, was trained with nodes sharing information between neighborings nodes. This allows
    connections to be created between similiar ailments, along with their symptoms, testing, and monitoring.

    1. The embeddings are compose of 67,124 clinical concepts that are encoded in 128-dimensional vectors. The KG maps these concepts into 7 discrete codes to categorize them. Together, they can support
        * chatbots to answer questions by making expertise-like connections spanning many different medical fields,
        * predicting level of disease on the patient level, and
        * clustering patients into different diseases subgroups to tailor and personalize for the best individual care.


1. This research can be extremely important for Humana to improve the health and well-being of its members and community since it can be utilized to tailor
personal patient care with expert level knowledge, leading waste and cost
reduction.

1. One way to include this research into the Q/A chatbot would be to
incorporate the embeddings into the Q/A chatbot to enhance the relevance
of the answers provided by improving the retrieval of relevent snippets of the publication in context.

    Ideally, and given more time, utilizing the KG with some additional input
    of patient's background would help personalize the experience
    and provide

# Choice for Base LLM
The base LLM for this chatbot was `deepseek-r1`. The choice was not just due to
its hype and being light weight, but also it provides some insight into its
"thinking" process. Within every response are `<think/>` tags that expose how
the LLM's reasoning, which includes interpretation of the question, citation
of source material (especially important when using RAG with source documents),
and logic that leads to the answer.

# Prerequisites
* python >=3.12<3.13
* Docker

# Quickstart
1. Install dependencies:
    ```bash
    make
    ```
1. Set up Ollama container: Pick **one** option below:
    1. If your machine has the means, you can run the Ollama container locally
    by building image and running the Ollama container by running the following
    command:
        ```bash
        make ollama
        ```
    1. **Recommended**: Deploy the container on cloud compute. I used GCP Cloud
    Run with the official Ollama image.
        * Make sure to ssh in and pull the `deepseek-r1` model, or any other
        model of your choice.

1. Rename the `.env_example` file to `.env`. If you picked option 1 in step 1
above, then you can leave the ollama server as is:
    ```
    OLLAMA_SERVER_URL="http://localhost:11434"
    ```

    Otherwise, you'll need to paste in the URL generated by your cloud provider
    there.

1. Run the streamlit web app!
```
streamlit run app.py
```



## Getting Started (DEV)
To setup and install dependencies, run the following command in the project root directory:

### Docker (recommended)
Set up the develop environment running

```bash
make setup_dev_env
```

This will build the development image, subsequently create and run the container in detached mode.

To attach to the container, and run
```bash
docker attach context_care_dev_inst
```
This will attach to the interactive terminal in the container


```bash
make setup
```
This will install a virtual environment to develop and/or run the app.

### Makefile

### Docker
Use the Docker image as your environment. Check to see if the the `Dockerfile` has all that is needed for development.
To develop, build the `dev` image by running:

```bash
docker compose build dev
```

Now that the image has been built, run the container with

```bash
docker compose run dev
```

to run the `dev` container
