{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f013e2c",
   "metadata": {},
   "source": [
    "# Step 2: Extract Data from PDF\n",
    "The first step is to extract the text/data from the PDF.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Please [download](https://www.researchgate.net/profile/Gary-Clark/publication/19364043_Slamon_DJ_Clark_GM_Wong_SG_Levin_WJ_Ullrich_A_McGuire_WLHuman_breast_cancer_correlation_of_relapse_and_survival_with_amplification_of_the_HER-2neu_oncogene_Science_Wash_DC_235_177-182/links/0046352b85f241a532000000/Slamon-DJ-Clark-GM-Wong-SG-Levin-WJ-Ullrich-A-McGuire-WLHuman-breast-cancer-correlation-of-relapse-and-survival-with-amplification-of-the-HER-2-neu-oncogene-Science-Wash-DC-235-177-182.pdf) and place PDF with the name unchanged \n",
    "(`SlamonetalSCIENCE1987.pdf`) in the `data/` directory within this project's root.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2301d6",
   "metadata": {},
   "source": [
    "# Load\n",
    "After trying few different off the shelf loaders, the one that looked the most\n",
    "reliable was `PyMuPDFLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dpath = Path.cwd() / \"data\"\n",
    "fname = \"SlamonetalSCIENCE1987.pdf\"\n",
    "\n",
    "fpath = dpath / fname\n",
    "\n",
    "# pages = pymupdf.open(fpath, filetype=\"txt\")\n",
    "\n",
    "# loader = PyPDFLoader(fpath)\n",
    "loader = PyMuPDFLoader(fpath)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d3dc3",
   "metadata": {},
   "source": [
    "**Note**: When opening and viewing the PDF, you'll notice that the content of \n",
    "the article is entirely within the 3rd (index 2) and the second to last page \n",
    "(index -1):\n",
    "1. The content of the first page (index 0) is just the title, authors, and some\n",
    "metadata that's not relevant to the context we need.\n",
    "1. The second page (index 1) just has some logos, DOI, and other related \n",
    "articles that are not directly in the content of the article.\n",
    "1. The last page (index -1) is another article entirely with the \n",
    "`References and Notes` section extended from the main article.\n",
    "\n",
    "For this reason, we're only going to extract the relevant information from those\n",
    "pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e8967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages = len(documents)\n",
    "content = [doc for doc in documents if doc.metadata[\"page\"] not in [0, 1, n_pages - 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35781b1",
   "metadata": {},
   "source": [
    "# Split Documents\n",
    "\n",
    "For performance and memory constraints, documents are split into chunks. This\n",
    "allows for quick retrieval of snippets to find the best answer for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e110f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=1_024, chunk_overlap=32, separator=\"\\n\")\n",
    "\n",
    "split = splitter.split_documents(content)\n",
    "split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971abeb",
   "metadata": {},
   "source": [
    "# Construct Vector Store\n",
    "Create vector store based on embeddings and document chunks. For simplicity,\n",
    "let's use the same base model for embedding.\n",
    "\n",
    "First we construct the vector store from the document chunks and embedding. Then\n",
    "we save it locally (TODO: Create database to host vector store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f386397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "VECTORSTORE_PATH = Path(\"data\") / \"vectorstore\" / \"db_pdf_context\"\n",
    "MODEL_NAME = \"deepseek-r1:latest\"\n",
    "OLLAMA_SERVER_URL = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "EMBEDDINGS = OllamaEmbeddings(model=MODEL_NAME, base_url=OLLAMA_SERVER_URL)\n",
    "# LLM = Ollama(model=MODEL_NAME, base_url=OLLAMA_SERVER_URL)\n",
    "vectorstore = FAISS.from_documents(split, EMBEDDINGS)\n",
    "vectorstore.save_local(VECTORSTORE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1854c",
   "metadata": {},
   "source": [
    "Let's give it a query and see what the vector store retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d03281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HER-2/neu amplification with various disease parameters were performed by the x2\\ntest. P values werc computed after combining the 5 to 20 and >20 cases, since there\\n-\\nwere so few samples in the >20 group.\\ni8o\\n on January 15, 2007 \\nwww.sciencemag.org\\nDownloaded from \\nBIOM 255 (Leffert) â€“ Discussion Feb. 1, 2007'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ad4df",
   "metadata": {},
   "source": [
    "Not super relevant, but asking irrelevant questions could be useful in \n",
    "evaluating our chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411062ce",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "If there was more time, it would be good to do some text cleaning since there \n",
    "are some issues (some misspellings/typos) in the text extraction in the OCR \n",
    "step. There are some extra spaces so some individual tokens appear to multiple \n",
    "words. If this is common, and on key words, then garbage in becomes garbage out.\n",
    "\n",
    "Luckily the chatbot's responses look good so revisiting this is low priority.\n",
    "\n",
    "Now that we understand the vector store, let's return to the chatbot app and \n",
    "ask some questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58898a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
